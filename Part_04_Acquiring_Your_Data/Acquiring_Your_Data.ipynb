{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquisition is the first critical step in the data analytics process. It involves gathering raw data from multiple sources, then transforming it into a format suitable for further analysis and processing. Understanding this process is essential because the quality, structure, and relevance of the data directly influence the outcomes of any analytics project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquisition is the method of collecting, measuring, and analyzing information from various sources. In the context of data analytics, it means gathering data from:\n",
    "- **Internal data**\n",
    "- **APIs**: These allow you to retrieve data in real time from online services like weather information, financial markets, social media platforms, and more.\n",
    "- **Online Datasets**: Repositories such as Kaggle, UCI Machine Learning Repository, and governmental portals offer pre-curated datasets in multiple formats (CSV, JSON, XML, Excel, etc.).\n",
    "- **Web Scraping**: When the required data is available on websites, web scraping techniques can be employed to extract unstructured data directly from HTML pages.\n",
    "- **Databases**: Structured data stored in relational databases (e.g., MySQL, PostgreSQL, SQLite) or NoSQL databases can be accessed using query languages like SQL or through specialized connectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four methods of acquiring data: \n",
    "- collecting new data; \n",
    "- converting/transforming legacy data; \n",
    "- sharing/exchanging data; \n",
    "- and purchasing data. \n",
    "\n",
    "<img src=\"https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/styles/side_image/public/thumbnails/image/DataAcquisitionVennDiagram.jpg?itok=zqYml3K-\" width=\"340\" height=\"340\">\n",
    "\n",
    "Source: https://www.usgs.gov/data-management/data-acquisition-methods\n",
    "\n",
    "This includes automated collection (e.g., of sensor-derived data), the manual recording of empirical observations, and obtaining existing data from other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Is Data Acquisition Important?**\n",
    "- Foundation for Analysis: The accuracy and reliability of your insights largely depend on the quality of the input data. Inaccurate or poorly formatted data can lead to misleading conclusions.\n",
    "- Diverse Data Sources: Modern analytics often require the integration of data from multiple sources. A well-designed data acquisition process ensures that disparate data can be merged, cleaned, and analyzed seamlessly.\n",
    "- Automation and Reproducibility: Automating data acquisition workflows (using scripts, scheduled jobs, or ETL tools) not only saves time but also makes the data analytics process more reproducible and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Process of Data Acquisition**\n",
    "\n",
    "**1. Identify Data Sources:** Determine where the necessary data resides. This could be external sources like APIs or web pages, or internal systems such as databases or log files.\n",
    "\n",
    "**2. Extraction:** Use the appropriate tools and techniques to extract the data. For instance, employ Python libraries like requests for API calls, BeautifulSoup or Scrapy for web scraping, and pandas or SQL connectors for databases.\n",
    "\n",
    "**3. Data Cleaning & Transformation:** Raw data is rarely analysis-ready. It often contains missing values, inconsistencies, or errors. Cleaning involves removing or imputing missing data, normalizing formats, and transforming the data to make it consistent across sources.\n",
    "\n",
    "**4. Integration:** When data comes from multiple sources, it must be merged into a coherent dataset. This involves aligning different data formats, handling duplicates, and ensuring that the integrated data preserves its integrity.\n",
    "\n",
    "**5. Storage:** Once cleaned and integrated, data is typically stored in formats that are optimal for analysis, such as CSV files, SQL databases, Excel Tables, JSON..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges in Data Acquisition**\n",
    "- **Data Quality Issues**: Incomplete, inconsistent, or outdated data can skew analysis. It is crucial to validate data accuracy at the point of collection.\n",
    "    - Missing values, duplicates, or inconsistent formatting (e.g., dates as MM/DD/YYYY vs. DD-MM-YYYY).\n",
    "    - Example: A survey dataset where 30% of respondents skipped income-level fields.\n",
    "    - Data may come in incompatible formats (e.g., API returns XML, but your tool expects JSON).\n",
    "\n",
    "- **Data Volume**: As data sources grow, handling large datasets efficiently becomes a challenge, requiring techniques for optimizing memory usage and processing speed.\n",
    "    - Handling large datasets (e.g., 10GB CSV files) may crash standard tools.\n",
    "\n",
    "- **Legal and Ethical Concerns**: Some data sources have strict usage policies or privacy restrictions. It is important to adhere to legal guidelines (e.g., GDPR) and respect website terms when scraping data.\n",
    "    - GDPR/CCPA Compliance: Ensure personal data is anonymized.\n",
    "    - Web Scraping Ethics: Respect robots.txt, avoid overloading servers.\n",
    "\n",
    "- **Integration Complexity**: Merging data from multiple formats and sources can lead to complications in maintaining data consistency and resolving conflicts between different data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Data Sources**\n",
    "\n",
    "- **Structured Data:**\n",
    "    - Definition: Organized in predefined formats (tables, rows, columns).\n",
    "    - Examples:\n",
    "        - Relational databases (MySQL, PostgreSQL).\n",
    "        - CSV/Excel files.\n",
    "    - Pros: Easy to query and analyze.\n",
    "    - Cons: Limited flexibility for complex/nested data.\n",
    "\n",
    "- **Semi-Structured Data:**\n",
    "    - Definition: Loosely organized with tags or markers (no strict schema).\n",
    "    - Examples:\n",
    "        - JSON (API responses), XML (web feeds), log files.\n",
    "    - Pros: Flexible for hierarchical/nested data.\n",
    "    - Cons: Requires parsing to extract meaning (e.g., nested JSON keys).\n",
    "\n",
    "- **Unstructured Data:**\n",
    "    - Definition: No predefined format; often text-heavy or multimedia.\n",
    "    - Examples:\n",
    "        - Social media posts, images, audio files, PDFs.\n",
    "    - Pros: Rich in insights (e.g., sentiment from text).\n",
    "    - Cons: Requires advanced tools (NLP, computer vision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Flat Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flat files store data in plain text or tabular formats without complex hierarchies. They are widely used for data exchange due to their simplicity and compatibility. Below is a comparison of common formats:\n",
    "\n",
    "<table><thead><tr><th><strong>Format</strong></th><th><strong>Structure</strong></th><th><strong>Pros</strong></th><th><strong>Cons</strong></th><th><strong>Use Cases</strong></th></tr></thead><tbody><tr><td><strong>CSV</strong></td><td>Comma-separated values</td><td>Lightweight, universal support</td><td>No data types, no hierarchy</td><td>Exporting SQL tables, raw data</td></tr><tr><td><strong>Excel</strong></td><td>Spreadsheets (rows/columns)</td><td>Supports formulas, multiple sheets</td><td>Proprietary, slow with large data</td><td>Manual data entry, reporting</td></tr><tr><td><strong>JSON</strong></td><td>Key-value pairs (nested)</td><td>Hierarchical, flexible schema</td><td>Verbose, harder to parse</td><td>APIs, web data</td></tr><tr><td><strong>XML</strong></td><td>Tag-based markup</td><td>Standardized, supports metadata</td><td>Bulky syntax, complex parsing</td><td>Legacy systems, config files</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text encoding: ASCII, Unicode, UTF-8\n",
    "\n",
    "- [The Absolute Minimum Every Software Developer Must Know About Unicode in 2023](https://tonsky.me/blog/unicode/)\n",
    "- [Unicode is harder than you think](https://mcilloni.ovh/2023/07/23/unicode-is-hard/)\n",
    "\n",
    "Text encoding is the process of converting characters (letters, numbers, symbols) into a sequence of bytes that computers can store, process, and transmit. Since computers fundamentally operate with binary data, encoding serves as the bridge between human-readable text and machine-readable code.\n",
    "\n",
    "In the ASCII encoding, which has 128 characters, only 95 of which are printable. The good news about ASCII encoding is that it’s the lowest common denominator of most data exchange. The bad news is that it doesn’t begin to handle the complexities of the many alphabets and writing systems of the world. Reading files using ASCII encoding is almost certain to cause trouble and throw errors on character values that it doesn’t understand, whether it’s a German ü, a Portuguese ç, or something from almost any language other than English.\n",
    "\n",
    "One way to mitigate this confusion is Unicode. The Unicode encoding called UTF-8 accepts the basic ASCII characters without any change but also allows an almost unlimited set of other characters and symbols according to the Unicode standard.\n",
    "\n",
    "Because of its flexibility, UTF-8 was used in more 85% of web pages served at the time I wrote this chapter, which means that your best bet for reading text files is to assume UTF-8 encoding. If the files contain only ASCII characters, they’ll still be read correctly, but you’ll also be covered if other characters are encoded in UTF-8. The good news is that the Python 3 string data type was designed to handle Unicode by default.\n",
    "\n",
    "Even with Unicode, there’ll be occasions when your text contains values that can’t be successfully encoded. Fortunately, the open function in Python accepts an optional errors parameter that tells it how to deal with encoding errors when reading or writing files. The default option is 'strict', which causes an error to be raised whenever an encoding error is encountered. Other useful options are 'ignore', which causes the character causing the error to be skipped; 'replace', which causes the character to be replaced by a marker character (often, ?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code results in a file that contains “ABC” followed by three non-ASCII characters, which may be rendered differently depending on the encoding used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out2.txt\", \"wb\") as f:\n",
    "    f.write(bytes([65, 66, 67, 255, 192, 193]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC���\n"
     ]
    }
   ],
   "source": [
    "! powershell cat out2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 3: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout2.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m<frozen codecs>:325\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 3: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth byte, which had a value of 255, isn’t a valid UTF-8 character in that position, so the 'strict' errors setting raises an exception. Now see how the other error options handle the same file, keeping in mind that the last three characters raise an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC\n"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\", errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC���\n"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\", errors=\"replace\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC\\xff\\xc0\\xc1\n"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\", errors=\"backslashreplace\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want any problem characters to disappear, 'ignore' is the option to use. The 'replace' option only marks the place occupied by the invalid character, and the other options in different ways attempt to preserve the invalid characters without interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most Western Windows installations, this will return \"cp1252\". However, note that the actual default encoding can vary depending on the system’s locale settings. Essentially, Python uses the result of locale.getpreferredencoding(False) as the default encoding for file operations when none is explicitly provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp1252\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "\n",
    "print(locale.getpreferredencoding(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCÿÀÁ\n"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the file\n",
    "! powershell rm out2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing Data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pandas I/O API** is a set of top level `reader` functions accessed like `pandas.read_csv()` that generally return a pandas object. The corresponding `writer` functions are object methods that are accessed like `DataFrame.to_csv()`. Below is a table containing available readers and writers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table\">\n",
    "<colgroup>\n",
    "<col style=\"width: 12.0%\">\n",
    "<col style=\"width: 40.0%\">\n",
    "<col style=\"width: 24.0%\">\n",
    "<col style=\"width: 24.0%\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Format Type</p></th>\n",
    "<th class=\"head\"><p>Data Description</p></th>\n",
    "<th class=\"head\"><p>Reader</p></th>\n",
    "<th class=\"head\"><p>Writer</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Comma-separated_values\">CSV</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-csv-table\"><span class=\"std std-ref\">read_csv</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-store-in-csv\"><span class=\"std std-ref\">to_csv</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p>Fixed-Width Text File</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-fwf-reader\"><span class=\"std std-ref\">read_fwf</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://www.json.org/\">JSON</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-json-reader\"><span class=\"std std-ref\">read_json</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-json-writer\"><span class=\"std std-ref\">to_json</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-html\"><span class=\"std std-ref\">read_html</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-html\"><span class=\"std std-ref\">to_html</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/LaTeX\">LaTeX</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-latex\"><span class=\"std std-ref\">Styler.to_latex</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://www.w3.org/standards/xml/core\">XML</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-xml\"><span class=\"std std-ref\">read_xml</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-xml\"><span class=\"std std-ref\">to_xml</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p>Local clipboard</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-clipboard\"><span class=\"std std-ref\">read_clipboard</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-clipboard\"><span class=\"std std-ref\">to_clipboard</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Microsoft_Excel\">MS Excel</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-excel-reader\"><span class=\"std std-ref\">read_excel</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-excel-writer\"><span class=\"std std-ref\">to_excel</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"http://opendocumentformat.org\">OpenDocument</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-ods\"><span class=\"std std-ref\">read_excel</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://support.hdfgroup.org/HDF5/whatishdf5.html\">HDF5 Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-hdf5\"><span class=\"std std-ref\">read_hdf</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-hdf5\"><span class=\"std std-ref\">to_hdf</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://github.com/wesm/feather\">Feather Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-feather\"><span class=\"std std-ref\">read_feather</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-feather\"><span class=\"std std-ref\">to_feather</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://parquet.apache.org/\">Parquet Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-parquet\"><span class=\"std std-ref\">read_parquet</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-parquet\"><span class=\"std std-ref\">to_parquet</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://orc.apache.org/\">ORC Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-orc\"><span class=\"std std-ref\">read_orc</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-orc\"><span class=\"std std-ref\">to_orc</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stata\">Stata</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-stata-reader\"><span class=\"std std-ref\">read_stata</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-stata-writer\"><span class=\"std std-ref\">to_stata</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SAS_(software)\">SAS</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sas-reader\"><span class=\"std std-ref\">read_sas</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SPSS\">SPSS</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-spss-reader\"><span class=\"std std-ref\">read_spss</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html\">Python Pickle Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-pickle\"><span class=\"std std-ref\">read_pickle</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-pickle\"><span class=\"std std-ref\">to_pickle</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>SQL</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SQL\">SQL</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sql\"><span class=\"std std-ref\">read_sql</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sql\"><span class=\"std std-ref\">to_sql</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>SQL</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/BigQuery\">Google BigQuery</a>;:ref:<cite>read_gbq&lt;io.bigquery&gt;</cite>;:ref:<cite>to_gbq&lt;io.bigquery&gt;</cite></p></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online datasets are pre-collected data available in various formats and hosted on platforms dedicated to data sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data from APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Fetch data from REST APIs and parse results into pandas.\n",
    "Topics:\n",
    "\n",
    "REST API fundamentals (endpoints, methods, status codes)\n",
    "\n",
    "Authentication: API keys, OAuth 2.0\n",
    "\n",
    "Using requests library: GET/POST, headers, pagination\n",
    "\n",
    "Handling rate limits and error codes\n",
    "Tutorial:\n",
    "\n",
    "Fetch weather data from OpenWeatherMap API.\n",
    "\n",
    "Paginate through GitHub Issues API and combine results.\n",
    "Exercise: Build a DataFrame of trending YouTube videos using YouTube Data API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Extract structured data from websites.\n",
    "Topics:\n",
    "\n",
    "HTML/CSS basics: tags, classes, IDs\n",
    "\n",
    "Ethical scraping: robots.txt, rate limiting\n",
    "\n",
    "Tools: BeautifulSoup, requests, pandas.read_html()\n",
    "Tutorial:\n",
    "\n",
    "Scrape Wikipedia tables into DataFrames with pd.read_html().\n",
    "\n",
    "Use BeautifulSoup to extract product prices from an e-commerce site.\n",
    "Exercise: Scrape real estate listings and calculate average prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many organizations store data in databases, which can be queried to extract exactly the information needed.\n",
    "\n",
    "Types of Databases\n",
    "- Relational Databases: Use SQL for querying (e.g., MySQL, PostgreSQL, SQLite).\n",
    "- NoSQL Databases: Designed for unstructured data (e.g., MongoDB, Cassandra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data & Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Access large datasets from cloud storage.\n",
    "Topics:\n",
    "\n",
    "Cloud storage: AWS S3, Google Cloud Storage\n",
    "\n",
    "Parquet/Feather formats for efficient I/O\n",
    "\n",
    "Using boto3 to read from S3\n",
    "Tutorial:\n",
    "\n",
    "Load a 1GB Parquet file from S3 using s3fs and pd.read_parquet().\n",
    "\n",
    "Compare read speeds for CSV vs. Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Data Acquisition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Combine multiple data sources into a unified dataset.\n",
    "Requirements:\n",
    "\n",
    "Scrape product data from a website.\n",
    "\n",
    "Fetch complementary pricing data via API.\n",
    "\n",
    "Merge with historical sales data from a SQL database.\n",
    "\n",
    "Validate, clean, and export to Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
