{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquiring Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquisition is the first critical step in the data analytics process. It involves gathering raw data from multiple sources, then transforming it into a format suitable for further analysis and processing. Understanding this process is essential because the quality, structure, and relevance of the data directly influence the outcomes of any analytics project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data acquisition is the method of collecting, measuring, and analyzing information from various sources. In the context of data analytics, it means gathering data from:\n",
    "- **Internal data**\n",
    "- **APIs**: These allow you to retrieve data in real time from online services like weather information, financial markets, social media platforms, and more.\n",
    "- **Online Datasets**: Repositories such as Kaggle, UCI Machine Learning Repository, and governmental portals offer pre-curated datasets in multiple formats (CSV, JSON, XML, Excel, etc.).\n",
    "- **Web Scraping**: When the required data is available on websites, web scraping techniques can be employed to extract unstructured data directly from HTML pages.\n",
    "- **Databases**: Structured data stored in relational databases (e.g., MySQL, PostgreSQL, SQLite) or NoSQL databases can be accessed using query languages like SQL or through specialized connectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four methods of acquiring data: \n",
    "- collecting new data; \n",
    "- converting/transforming legacy data; \n",
    "- sharing/exchanging data; \n",
    "- and purchasing data. \n",
    "\n",
    "<img src=\"https://d9-wret.s3.us-west-2.amazonaws.com/assets/palladium/production/s3fs-public/styles/side_image/public/thumbnails/image/DataAcquisitionVennDiagram.jpg?itok=zqYml3K-\" width=\"340\" height=\"340\">\n",
    "\n",
    "Source: https://www.usgs.gov/data-management/data-acquisition-methods\n",
    "\n",
    "This includes automated collection (e.g., of sensor-derived data), the manual recording of empirical observations, and obtaining existing data from other sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why Is Data Acquisition Important?**\n",
    "- Foundation for Analysis: The accuracy and reliability of your insights largely depend on the quality of the input data. Inaccurate or poorly formatted data can lead to misleading conclusions.\n",
    "- Diverse Data Sources: Modern analytics often require the integration of data from multiple sources. A well-designed data acquisition process ensures that disparate data can be merged, cleaned, and analyzed seamlessly.\n",
    "- Automation and Reproducibility: Automating data acquisition workflows (using scripts, scheduled jobs, or ETL tools) not only saves time but also makes the data analytics process more reproducible and scalable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Process of Data Acquisition**\n",
    "\n",
    "**1. Identify Data Sources:** Determine where the necessary data resides. This could be external sources like APIs or web pages, or internal systems such as databases or log files.\n",
    "\n",
    "**2. Extraction:** Use the appropriate tools and techniques to extract the data. For instance, employ Python libraries like requests for API calls, BeautifulSoup or Scrapy for web scraping, and pandas or SQL connectors for databases.\n",
    "\n",
    "**3. Data Cleaning & Transformation:** Raw data is rarely analysis-ready. It often contains missing values, inconsistencies, or errors. Cleaning involves removing or imputing missing data, normalizing formats, and transforming the data to make it consistent across sources.\n",
    "\n",
    "**4. Integration:** When data comes from multiple sources, it must be merged into a coherent dataset. This involves aligning different data formats, handling duplicates, and ensuring that the integrated data preserves its integrity.\n",
    "\n",
    "**5. Storage:** Once cleaned and integrated, data is typically stored in formats that are optimal for analysis, such as CSV files, SQL databases, Excel Tables, JSON..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges in Data Acquisition**\n",
    "- **Data Quality Issues**: Incomplete, inconsistent, or outdated data can skew analysis. It is crucial to validate data accuracy at the point of collection.\n",
    "    - Missing values, duplicates, or inconsistent formatting (e.g., dates as MM/DD/YYYY vs. DD-MM-YYYY).\n",
    "    - Example: A survey dataset where 30% of respondents skipped income-level fields.\n",
    "    - Data may come in incompatible formats (e.g., API returns XML, but your tool expects JSON).\n",
    "\n",
    "- **Data Volume**: As data sources grow, handling large datasets efficiently becomes a challenge, requiring techniques for optimizing memory usage and processing speed.\n",
    "    - Handling large datasets (e.g., 10GB CSV files) may crash standard tools.\n",
    "\n",
    "- **Legal and Ethical Concerns**: Some data sources have strict usage policies or privacy restrictions. It is important to adhere to legal guidelines (e.g., GDPR) and respect website terms when scraping data.\n",
    "    - GDPR/CCPA Compliance: Ensure personal data is anonymized.\n",
    "    - Web Scraping Ethics: Respect robots.txt, avoid overloading servers.\n",
    "\n",
    "- **Integration Complexity**: Merging data from multiple formats and sources can lead to complications in maintaining data consistency and resolving conflicts between different data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Types of Data Sources**\n",
    "\n",
    "- **Structured Data:**\n",
    "    - Definition: Organized in predefined formats (tables, rows, columns).\n",
    "    - Examples:\n",
    "        - Relational databases (MySQL, PostgreSQL).\n",
    "        - CSV/Excel files.\n",
    "    - Pros: Easy to query and analyze.\n",
    "    - Cons: Limited flexibility for complex/nested data.\n",
    "\n",
    "- **Semi-Structured Data:**\n",
    "    - Definition: Loosely organized with tags or markers (no strict schema).\n",
    "    - Examples:\n",
    "        - JSON (API responses), XML (web feeds), log files.\n",
    "    - Pros: Flexible for hierarchical/nested data.\n",
    "    - Cons: Requires parsing to extract meaning (e.g., nested JSON keys).\n",
    "\n",
    "- **Unstructured Data:**\n",
    "    - Definition: No predefined format; often text-heavy or multimedia.\n",
    "    - Examples:\n",
    "        - Social media posts, images, audio files, PDFs.\n",
    "    - Pros: Rich in insights (e.g., sentiment from text).\n",
    "    - Cons: Requires advanced tools (NLP, computer vision)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Flat Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flat files store data in plain text or tabular formats without complex hierarchies. They are widely used for data exchange due to their simplicity and compatibility. Below is a comparison of common formats:\n",
    "\n",
    "<table><thead><tr><th><strong>Format</strong></th><th><strong>Structure</strong></th><th><strong>Pros</strong></th><th><strong>Cons</strong></th><th><strong>Use Cases</strong></th></tr></thead><tbody><tr><td><strong>CSV</strong></td><td>Comma-separated values</td><td>Lightweight, universal support</td><td>No data types, no hierarchy</td><td>Exporting SQL tables, raw data</td></tr><tr><td><strong>Excel</strong></td><td>Spreadsheets (rows/columns)</td><td>Supports formulas, multiple sheets</td><td>Proprietary, slow with large data</td><td>Manual data entry, reporting</td></tr><tr><td><strong>JSON</strong></td><td>Key-value pairs (nested)</td><td>Hierarchical, flexible schema</td><td>Verbose, harder to parse</td><td>APIs, web data</td></tr><tr><td><strong>XML</strong></td><td>Tag-based markup</td><td>Standardized, supports metadata</td><td>Bulky syntax, complex parsing</td><td>Legacy systems, config files</td></tr></tbody></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text encoding: ASCII, Unicode, UTF-8\n",
    "\n",
    "- [The Absolute Minimum Every Software Developer Must Know About Unicode in 2023](https://tonsky.me/blog/unicode/)\n",
    "- [Unicode is harder than you think](https://mcilloni.ovh/2023/07/23/unicode-is-hard/)\n",
    "\n",
    "Text encoding is the process of converting characters (letters, numbers, symbols) into a sequence of bytes that computers can store, process, and transmit. Since computers fundamentally operate with binary data, encoding serves as the bridge between human-readable text and machine-readable code.\n",
    "\n",
    "In the ASCII encoding, which has 128 characters, only 95 of which are printable. The good news about ASCII encoding is that it’s the lowest common denominator of most data exchange. The bad news is that it doesn’t begin to handle the complexities of the many alphabets and writing systems of the world. Reading files using ASCII encoding is almost certain to cause trouble and throw errors on character values that it doesn’t understand, whether it’s a German ü, a Portuguese ç, or something from almost any language other than English.\n",
    "\n",
    "One way to mitigate this confusion is Unicode. The Unicode encoding called UTF-8 accepts the basic ASCII characters without any change but also allows an almost unlimited set of other characters and symbols according to the Unicode standard.\n",
    "\n",
    "Because of its flexibility, UTF-8 was used in more 85% of web pages served at the time I wrote this chapter, which means that your best bet for reading text files is to assume UTF-8 encoding. If the files contain only ASCII characters, they’ll still be read correctly, but you’ll also be covered if other characters are encoded in UTF-8. The good news is that the Python 3 string data type was designed to handle Unicode by default.\n",
    "\n",
    "Even with Unicode, there’ll be occasions when your text contains values that can’t be successfully encoded. Fortunately, the open function in Python accepts an optional errors parameter that tells it how to deal with encoding errors when reading or writing files. The default option is 'strict', which causes an error to be raised whenever an encoding error is encountered. Other useful options are 'ignore', which causes the character causing the error to be skipped; 'replace', which causes the character to be replaced by a marker character (often, ?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code results in a file that contains “ABC” followed by three non-ASCII characters, which may be rendered differently depending on the encoding used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out2.txt\", \"wb\") as f:\n",
    "    f.write(bytes([65, 66, 67, 255, 192, 193]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC���\n"
     ]
    }
   ],
   "source": [
    "! powershell cat out2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xff in position 3: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout2.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m<frozen codecs>:325\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xff in position 3: invalid start byte"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth byte, which had a value of 255, isn’t a valid UTF-8 character in that position, so the 'strict' errors setting raises an exception. Now see how the other error options handle the same file, keeping in mind that the last three characters raise an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC\n"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\", errors=\"ignore\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC���\n"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\", errors=\"replace\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC\\xff\\xc0\\xc1\n"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\", errors=\"backslashreplace\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want any problem characters to disappear, 'ignore' is the option to use. The 'replace' option only marks the place occupied by the invalid character, and the other options in different ways attempt to preserve the invalid characters without interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most Western Windows installations, this will return \"cp1252\". However, note that the actual default encoding can vary depending on the system’s locale settings. Essentially, Python uses the result of locale.getpreferredencoding(False) as the default encoding for file operations when none is explicitly provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp1252\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "\n",
    "print(locale.getpreferredencoding(False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCÿÀÁ\n"
     ]
    }
   ],
   "source": [
    "with open(\"out2.txt\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the file\n",
    "! powershell rm out2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing Data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pandas I/O API** is a set of top level `reader` functions accessed like `pandas.read_csv()` that generally return a pandas object. The corresponding `writer` functions are object methods that are accessed like `DataFrame.to_csv()`. Below is a table containing available readers and writers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"table\">\n",
    "<colgroup>\n",
    "<col style=\"width: 12.0%\">\n",
    "<col style=\"width: 40.0%\">\n",
    "<col style=\"width: 24.0%\">\n",
    "<col style=\"width: 24.0%\">\n",
    "</colgroup>\n",
    "<thead>\n",
    "<tr class=\"row-odd\"><th class=\"head\"><p>Format Type</p></th>\n",
    "<th class=\"head\"><p>Data Description</p></th>\n",
    "<th class=\"head\"><p>Reader</p></th>\n",
    "<th class=\"head\"><p>Writer</p></th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Comma-separated_values\">CSV</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-csv-table\"><span class=\"std std-ref\">read_csv</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-store-in-csv\"><span class=\"std std-ref\">to_csv</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p>Fixed-Width Text File</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-fwf-reader\"><span class=\"std std-ref\">read_fwf</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://www.json.org/\">JSON</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-json-reader\"><span class=\"std std-ref\">read_json</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-json-writer\"><span class=\"std std-ref\">to_json</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-html\"><span class=\"std std-ref\">read_html</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-html\"><span class=\"std std-ref\">to_html</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/LaTeX\">LaTeX</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-latex\"><span class=\"std std-ref\">Styler.to_latex</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>text</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://www.w3.org/standards/xml/core\">XML</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-read-xml\"><span class=\"std std-ref\">read_xml</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-xml\"><span class=\"std std-ref\">to_xml</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>text</p></td>\n",
    "<td><p>Local clipboard</p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-clipboard\"><span class=\"std std-ref\">read_clipboard</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-clipboard\"><span class=\"std std-ref\">to_clipboard</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Microsoft_Excel\">MS Excel</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-excel-reader\"><span class=\"std std-ref\">read_excel</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-excel-writer\"><span class=\"std std-ref\">to_excel</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"http://opendocumentformat.org\">OpenDocument</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-ods\"><span class=\"std std-ref\">read_excel</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://support.hdfgroup.org/HDF5/whatishdf5.html\">HDF5 Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-hdf5\"><span class=\"std std-ref\">read_hdf</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-hdf5\"><span class=\"std std-ref\">to_hdf</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://github.com/wesm/feather\">Feather Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-feather\"><span class=\"std std-ref\">read_feather</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-feather\"><span class=\"std std-ref\">to_feather</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://parquet.apache.org/\">Parquet Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-parquet\"><span class=\"std std-ref\">read_parquet</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-parquet\"><span class=\"std std-ref\">to_parquet</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://orc.apache.org/\">ORC Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-orc\"><span class=\"std std-ref\">read_orc</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-orc\"><span class=\"std std-ref\">to_orc</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/Stata\">Stata</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-stata-reader\"><span class=\"std std-ref\">read_stata</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-stata-writer\"><span class=\"std std-ref\">to_stata</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SAS_(software)\">SAS</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sas-reader\"><span class=\"std std-ref\">read_sas</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SPSS\">SPSS</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-spss-reader\"><span class=\"std std-ref\">read_spss</span></a></p></td>\n",
    "<td><p>NA</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>binary</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://docs.python.org/3/library/pickle.html\">Python Pickle Format</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-pickle\"><span class=\"std std-ref\">read_pickle</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-pickle\"><span class=\"std std-ref\">to_pickle</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p>SQL</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/SQL\">SQL</a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sql\"><span class=\"std std-ref\">read_sql</span></a></p></td>\n",
    "<td><p><a class=\"reference internal\" href=\"#io-sql\"><span class=\"std std-ref\">to_sql</span></a></p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p>SQL</p></td>\n",
    "<td><p><a class=\"reference external\" href=\"https://en.wikipedia.org/wiki/BigQuery\">Google BigQuery</a></p></td>\n",
    "<td></td>\n",
    "<td></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_folder_path = Path.cwd().parent / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>0.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99</td>\n",
       "      <td>0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time  Percent\n",
       "0    99    0.067\n",
       "1    99    0.133\n",
       "2    99    0.067\n",
       "3    99    0.000\n",
       "4    99    0.000\n",
       "5     0    0.500\n",
       "6     0    0.467\n",
       "7     0    0.857\n",
       "8     0    0.500\n",
       "9     0    0.357"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 1\n",
    "data = pd.read_csv(data_folder_path / \"seaslug.txt\", sep=\"\\t\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sep`: str, defaults to ',' for read_csv(), \\t for read_table(): Delimiter to use. If sep is None, the C engine cannot automatically detect the separator, but the Python parsing engine can, meaning the latter will be used and automatically detect the separator by Python’s builtin sniffer tool, csv.Sniffer. In addition, separators longer than 1 character and different from '\\s+' will be interpreted as regular expressions and will also force the use of the Python parsing engine. Note that regex delimiters are prone to ignoring quoted data. Regex example: '\\\\r\\\\t'.\n",
    "- `delimiter` str, default None: Alternative argument name for sep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>100</td>\n",
       "      <td>Butter, salted</td>\n",
       "      <td>BUTTER,WITH SALT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.38</td>\n",
       "      <td>4.27</td>\n",
       "      <td>8.79</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>100</td>\n",
       "      <td>Butter, whipped, with salt</td>\n",
       "      <td>BUTTER,WHIPPED,W/ SALT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1003</td>\n",
       "      <td>100</td>\n",
       "      <td>Butter oil, anhydrous</td>\n",
       "      <td>BUTTER OIL,ANHYDROUS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.38</td>\n",
       "      <td>4.27</td>\n",
       "      <td>8.79</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>100</td>\n",
       "      <td>Cheese, blue</td>\n",
       "      <td>CHEESE,BLUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.38</td>\n",
       "      <td>4.27</td>\n",
       "      <td>8.79</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1005</td>\n",
       "      <td>100</td>\n",
       "      <td>Cheese, brick</td>\n",
       "      <td>CHEESE,BRICK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.38</td>\n",
       "      <td>4.27</td>\n",
       "      <td>8.79</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1                           2                       3   4   5  6   \\\n",
       "0  1001  100              Butter, salted        BUTTER,WITH SALT NaN NaN  Y   \n",
       "1  1002  100  Butter, whipped, with salt  BUTTER,WHIPPED,W/ SALT NaN NaN  Y   \n",
       "2  1003  100       Butter oil, anhydrous    BUTTER OIL,ANHYDROUS NaN NaN  Y   \n",
       "3  1004  100                Cheese, blue             CHEESE,BLUE NaN NaN  Y   \n",
       "4  1005  100               Cheese, brick            CHEESE,BRICK NaN NaN  Y   \n",
       "\n",
       "   7   8   9     10    11    12    13  \n",
       "0 NaN   0 NaN  6.38  4.27  8.79  3.87  \n",
       "1 NaN   0 NaN  6.38   NaN   NaN   NaN  \n",
       "2 NaN   0 NaN  6.38  4.27  8.79  3.87  \n",
       "3 NaN   0 NaN  6.38  4.27  8.79  3.87  \n",
       "4 NaN   0 NaN  6.38  4.27  8.79  3.87  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 2: Encoding: `iso-8859-1`, separator: `^`\n",
    "data = pd.read_csv(data_folder_path / \"FOOD_DES.txt\", sep=\"^\", encoding=\"iso-8859-1\", header=None, nrows=5, quotechar=\"~\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `nrows: int, default None` Number of rows of file to read. Useful for reading pieces of large files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `header: int or list of ints, default 'infer'` Row number(s) to use as the column names, and the start of the data. Default behavior is to infer the column names: if no names are passed the behavior is identical to header=0 and column names are inferred from the first line of the file, if column names are passed explicitly then the behavior is identical to header=None. Explicitly pass header=0 to be able to replace existing names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `encoding: str, default None` Encoding to use for UTF when reading/writing (e.g. 'utf-8'). [List of Python standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `quotechar: str (length 1)`: The character used to denote the start and end of a quoted item. Quoted items can include the delimiter and it will be ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id Num', 'date', 'problem', 'MDC', 'citation Issued',\n",
       "       'person Search', 'vehicle Search', 'pre Race', 'race', 'gender', 'lat',\n",
       "       'long', 'police Precinct', 'neighborhood'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 3\n",
    "data = pd.read_csv(data_folder_path / \"mpls_stops.csv\", nrows=3)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['case_number_id', 'id_num', 'date', 'problem', 'mdc', 'citation_issued', 'person_search', 'vehicle_search', 'pre_race', 'race', 'gender', 'lat', 'long', 'police_precinct', 'neighborhood']\n"
     ]
    }
   ],
   "source": [
    "new_column_names = list(data.columns)\n",
    "new_column_names = [name.lower().replace(\" \", \"_\") for name in new_column_names]\n",
    "new_column_names[0] = \"case_number_id\"\n",
    "print(new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_num</th>\n",
       "      <th>date</th>\n",
       "      <th>problem</th>\n",
       "      <th>mdc</th>\n",
       "      <th>citation_issued</th>\n",
       "      <th>person_search</th>\n",
       "      <th>vehicle_search</th>\n",
       "      <th>pre_race</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>police_precinct</th>\n",
       "      <th>neighborhood</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_number_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6823</th>\n",
       "      <td>17-000003</td>\n",
       "      <td>2017-01-01 00:00:42</td>\n",
       "      <td>suspicious</td>\n",
       "      <td>MDC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.966617</td>\n",
       "      <td>-93.246458</td>\n",
       "      <td>1</td>\n",
       "      <td>Cedar Riverside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6824</th>\n",
       "      <td>17-000007</td>\n",
       "      <td>2017-01-01 00:03:07</td>\n",
       "      <td>suspicious</td>\n",
       "      <td>MDC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>44.980450</td>\n",
       "      <td>-93.271340</td>\n",
       "      <td>1</td>\n",
       "      <td>Downtown West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6825</th>\n",
       "      <td>17-000073</td>\n",
       "      <td>2017-01-01 00:23:15</td>\n",
       "      <td>traffic</td>\n",
       "      <td>MDC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>44.948350</td>\n",
       "      <td>-93.275380</td>\n",
       "      <td>5</td>\n",
       "      <td>Whittier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6826</th>\n",
       "      <td>17-000092</td>\n",
       "      <td>2017-01-01 00:33:48</td>\n",
       "      <td>suspicious</td>\n",
       "      <td>MDC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>East African</td>\n",
       "      <td>Male</td>\n",
       "      <td>44.948360</td>\n",
       "      <td>-93.281350</td>\n",
       "      <td>5</td>\n",
       "      <td>Whittier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6827</th>\n",
       "      <td>17-000098</td>\n",
       "      <td>2017-01-01 00:37:58</td>\n",
       "      <td>traffic</td>\n",
       "      <td>MDC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>44.979078</td>\n",
       "      <td>-93.262076</td>\n",
       "      <td>1</td>\n",
       "      <td>Downtown West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id_num                date     problem  mdc  \\\n",
       "case_number_id                                                   \n",
       "6823            17-000003 2017-01-01 00:00:42  suspicious  MDC   \n",
       "6824            17-000007 2017-01-01 00:03:07  suspicious  MDC   \n",
       "6825            17-000073 2017-01-01 00:23:15     traffic  MDC   \n",
       "6826            17-000092 2017-01-01 00:33:48  suspicious  MDC   \n",
       "6827            17-000098 2017-01-01 00:37:58     traffic  MDC   \n",
       "\n",
       "               citation_issued person_search vehicle_search pre_race  \\\n",
       "case_number_id                                                         \n",
       "6823                       NaN         False          False      NaN   \n",
       "6824                       NaN         False          False      NaN   \n",
       "6825                       NaN         False          False      NaN   \n",
       "6826                       NaN         False          False      NaN   \n",
       "6827                       NaN         False          False      NaN   \n",
       "\n",
       "                        race  gender        lat       long  police_precinct  \\\n",
       "case_number_id                                                                \n",
       "6823                     NaN     NaN  44.966617 -93.246458                1   \n",
       "6824                     NaN    Male  44.980450 -93.271340                1   \n",
       "6825                   White  Female  44.948350 -93.275380                5   \n",
       "6826            East African    Male  44.948360 -93.281350                5   \n",
       "6827                   White  Female  44.979078 -93.262076                1   \n",
       "\n",
       "                   neighborhood  \n",
       "case_number_id                   \n",
       "6823            Cedar Riverside  \n",
       "6824              Downtown West  \n",
       "6825                   Whittier  \n",
       "6826                   Whittier  \n",
       "6827              Downtown West  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\n",
    "    data_folder_path / \"mpls_stops.csv\",\n",
    "    names=new_column_names,\n",
    "    skiprows=2,\n",
    "    engine=\"c\",\n",
    "    true_values=[\"YES\"],\n",
    "    false_values=[\"NO\"],\n",
    "    index_col=\"case_number_id\",\n",
    "    parse_dates=[\"date\"],\n",
    "    date_format=\"%Y-%m-%d %H:%M:%S\",\n",
    "    na_values=[\"Unknown\"],\n",
    "    dtype={\n",
    "        \"mdc\": \"category\",\n",
    "        \"problem\": \"category\",\n",
    "        \"pre_race\": \"category\",\n",
    "        \"race\": \"category\",\n",
    "        \"gender\": \"category\",\n",
    "        \"police_precinct\": \"int8\",\n",
    "        \"neighborhood\": \"category\",\n",
    "    },\n",
    ")\n",
    "data.index = data.index.astype(\"int\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 33198 entries, 6823 to 41399\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype         \n",
      "---  ------           --------------  -----         \n",
      " 0   id_num           33198 non-null  object        \n",
      " 1   date             33198 non-null  datetime64[ns]\n",
      " 2   problem          33198 non-null  category      \n",
      " 3   mdc              33198 non-null  category      \n",
      " 4   citation_issued  3656 non-null   object        \n",
      " 5   person_search    28242 non-null  object        \n",
      " 6   vehicle_search   28242 non-null  object        \n",
      " 7   pre_race         9828 non-null   category      \n",
      " 8   race             22821 non-null  category      \n",
      " 9   gender           24258 non-null  category      \n",
      " 10  lat              33198 non-null  float64       \n",
      " 11  long             33198 non-null  float64       \n",
      " 12  police_precinct  33198 non-null  int8          \n",
      " 13  neighborhood     33198 non-null  category      \n",
      "dtypes: category(6), datetime64[ns](1), float64(2), int8(1), object(4)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `names: array-like, default None` List of column names to use. If file contains no header row, then you should explicitly pass header=None. Duplicates in this list are not allowed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `skiprows: list-like or integer, default None` Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `engine: {‘c’, ‘python’, ‘pyarrow’}` Parser engine to use. The C and pyarrow engines are faster, while the python engine is currently more feature-complete. Multithreading is currently only supported by the pyarrow engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 1 mpls = pd.read_csv(data_folder_path / \"mpls_stops.csv\", names=new_column_names, skiprows=2, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.4 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 1 mpls = pd.read_csv(data_folder_path / \"mpls_stops.csv\", names=new_column_names, skiprows=2, engine='c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.9 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1 -r 1 mpls = pd.read_csv(data_folder_path / \"mpls_stops.csv\", names=new_column_names, skiprows=2, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `true_values: list, default None` Values to consider as True.\n",
    "- `false_values: list, default None` Values to consider as False.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `index_col: int, str, sequence of int / str, or False, default None` Column(s) to use as the row labels of the DataFrame, either given as string name or column index. If a sequence of int / str is given, a MultiIndex is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `dtype: Type name or dict of column -> type, default None` Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32} (unsupported with engine='python'). Use str or object together with suitable na_values settings to preserve and not interpret dtype.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `parse_dates: boolean or list of ints or names or list of lists or dict, default False.`\n",
    "  - If True -> try parsing the index.\n",
    "  - If [1, 2, 3] -> try parsing columns 1, 2, 3 each as a separate date column.\n",
    "  - If [[1, 3]] -> combine columns 1 and 3 and parse as a single date column.\n",
    "  - If {'foo': [1, 3]} -> parse columns 1, 3 as date and call result ‘foo’. A fast-path exists for iso8601-formatted dates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `date_format` Format to use for parsing dates when used in conjunction with parse_dates. The strftime to parse time, e.g. \"%d/%m/%Y\". See strftime documentation for more information on choices, though note that \"%f\" will parse all the way up to nanoseconds. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `na_values: scalar, str, list-like, or dict, default None` Additional strings to recognize as NA/NaN. If dict passed, specific per-column NA values. See na values const below for a list of the values interpreted as NaN by default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 15 19:35:11 CEST 2018\n",
      "Connecting to host x.x.x.x, port 5201\n",
      "[  4] local x.x.x.x port 48944 connected to x.x.x.x port 5201\n",
      "[ ID] Interval           Transfer     Bandwidth       Retr  Cwnd\n",
      "[  4]   0.00-1.00   sec   375 MBytes  3.14 Gbits/sec  273    471 KBytes\n",
      "[  4]   1.00-2.00   sec   428 MBytes  3.59 Gbits/sec  145    376 KBytes\n",
      "[  4]   2.00-3.00   sec   360 MBytes  3.02 Gbits/sec  148    454 KBytes\n",
      "[  4]   3.00-4.00   sec   339 MBytes  2.84 Gbits/sec   83    407 KBytes\n",
      "[  4]   4.00-5.00   sec   305 MBytes  2.56 Gbits/sec  104    414 KBytes\n",
      "[  4]   5.00-6.00   sec   301 MBytes  2.53 Gbits/sec  186    440 KBytes\n",
      "[  4]   6.00-7.00   sec   325 MBytes  2.73 Gbits/sec  174    485 KBytes\n",
      "[  4]   7.00-8.00   sec   434 MBytes  3.64 Gbits/sec   81    677 KBytes\n",
      "[  4]   8.00-9.00   sec   412 MBytes  3.46 Gbits/sec  226    537 KBytes\n",
      "[  4]   9.00-10.00  sec   409 MBytes  3.43 Gbits/sec   47    372 KBytes\n",
      "[  4]   10.00-11.00  sec   523 MBytes  3.81 Gbits/sec   96    422 KBytes\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "! powershell cat  ../data/iperf.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-08-15 19:35:11+00:00 <class 'datetime.datetime'>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import datetime\n",
    "\n",
    "file_path = data_folder_path / \"iperf.txt\"\n",
    "temp_file_path = data_folder_path / \"iperf_temp.txt\"\n",
    "\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "    raw_data = [line.strip() for line in raw_data]\n",
    "\n",
    "start_time = datetime.datetime.strptime(raw_data[0], \"%a %b %d %H:%M:%S CEST %Y\").replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "print(start_time, type(start_time))\n",
    "\n",
    "rows = []\n",
    "for line in raw_data[4:]:\n",
    "    line_splitted = line.split()\n",
    "    # seconds to add to start time\n",
    "    add_seconds = int(line_splitted[2].split(\".\")[0])\n",
    "    timestamp = start_time + datetime.timedelta(seconds=add_seconds)\n",
    "    transfer_mbytesec = int(line_splitted[4])\n",
    "    bandwidth_gbitsec = float(line_splitted[6])\n",
    "    retr = int(line_splitted[8])\n",
    "    cwnd_kbytes = int(line_splitted[9])\n",
    "    rows.append((timestamp, transfer_mbytesec, bandwidth_gbitsec, retr, cwnd_kbytes))\n",
    "\n",
    "\n",
    "headers = [\"timestamp\", \"transfer_mbytesec\", \"bandwidth_gbitsec\", \"retr\", \"cwnd_kbytes\"]\n",
    "\n",
    "with temp_file_path.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "data = pd.read_csv(temp_file_path, parse_dates=[\"timestamp\"], index_col=[\"timestamp\"])\n",
    "data.head(10)\n",
    "\n",
    "# remove the file\n",
    "temp_file_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 11 entries, 2018-08-15 19:35:11+00:00 to 2018-08-15 19:35:21+00:00\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   transfer_mbytesec  11 non-null     int64  \n",
      " 1   bandwidth_gbitsec  11 non-null     float64\n",
      " 2   retr               11 non-null     int64  \n",
      " 3   cwnd_kbytes        11 non-null     int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 440.0 bytes\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Datasets examples](https://github.com/jdorfman/awesome-json-datasets#bitcoinm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`pandas.read_json`](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html): pandas.read_json(path_or_buf, *, orient=None, typ='frame', dtype=None, convert_axes=None, convert_dates=True, keep_default_dates=True, precise_float=False, date_unit=None, encoding=None, encoding_errors='strict', lines=False, chunksize=None, compression='infer', nrows=None, storage_options=None, dtype_backend=<no_default>, engine='ujson')\n",
    "\n",
    "**Orient options**\n",
    "\n",
    "Default is `'columns'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"colwidths-given table\">\n",
    "<colgroup>\n",
    "<col style=\"width: 12%\">\n",
    "<col style=\"width: 88%\">\n",
    "</colgroup>\n",
    "<tbody>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">split</span></code></p></td>\n",
    "<td><p>dict like {index -&gt; [index], columns -&gt; [columns], data -&gt; [values]}</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">records</span></code></p></td>\n",
    "<td><p>list like [{column -&gt; value}, … , {column -&gt; value}]</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">index</span></code></p></td>\n",
    "<td><p>dict like {index -&gt; {column -&gt; value}}</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">columns</span></code></p></td>\n",
    "<td><p>dict like {column -&gt; {index -&gt; value}}</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">values</span></code></p></td>\n",
    "<td><p>just the values array</p></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><code class=\"docutils literal notranslate\"><span class=\"pre\">table</span></code></p></td>\n",
    "<td><p>adhering to the JSON <a class=\"reference external\" href=\"https://specs.frictionlessdata.io/json-table-schema/\">Table Schema</a></p></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C\n",
       "x  1  4  7\n",
       "y  2  5  8\n",
       "z  3  6  9"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame({\"A\": range(1, 4), \"B\": range(4, 7), \"C\": range(7, 10)}, columns=list(\"ABC\"), index=list(\"xyz\"))\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the JSON string:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Column oriented (the default for DataFrame) serializes the data as nested JSON objects with column labels acting as the primary index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"A\":{\"x\":1,\"y\":2,\"z\":3},\"B\":{\"x\":4,\"y\":5,\"z\":6},\"C\":{\"x\":7,\"y\":8,\"z\":9}}'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.to_json(orient=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Index oriented (the default for Series) similar to column oriented but the index labels are now primary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"x\":{\"A\":1,\"B\":4,\"C\":7},\"y\":{\"A\":2,\"B\":5,\"C\":8},\"z\":{\"A\":3,\"B\":6,\"C\":9}}'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.to_json(orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Record oriented serializes the data to a JSON array of column -> value records, index labels are not included. This is useful for passing DataFrame data to plotting libraries, for example the JavaScript library d3.js\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"A\":1,\"B\":4,\"C\":7},{\"A\":2,\"B\":5,\"C\":8},{\"A\":3,\"B\":6,\"C\":9}]'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Value oriented is a bare-bones option which serializes to nested JSON arrays of values only, column and index labels are not included:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[1,4,7],[2,5,8],[3,6,9]]'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.to_json(orient=\"values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split oriented serializes to a JSON object containing separate entries for values, index and columns. Name is also included for Series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"columns\":[\"A\",\"B\",\"C\"],\"index\":[\"x\",\"y\",\"z\"],\"data\":[[1,4,7],[2,5,8],[3,6,9]]}'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.to_json(orient=\"split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Table oriented serializes to the JSON Table Schema, allowing for the preservation of metadata including but not limited to dtypes and index names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"schema\":{\"fields\":[{\"name\":\"index\",\"type\":\"string\"},{\"name\":\"A\",\"type\":\"integer\"},{\"name\":\"B\",\"type\":\"integer\"},{\"name\":\"C\",\"type\":\"integer\"}],\"primaryKey\":[\"index\"],\"pandas_version\":\"1.4.0\"},\"data\":[{\"index\":\"x\",\"A\":1,\"B\":4,\"C\":7},{\"index\":\"y\",\"A\":2,\"B\":5,\"C\":8},{\"index\":\"z\",\"A\":3,\"B\":6,\"C\":9}]}'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.to_json(orient=\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp_anomaly_celsius</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>-0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>-0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>-0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>-0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      temp_anomaly_celsius\n",
       "year                      \n",
       "1880                 -0.12\n",
       "1881                 -0.09\n",
       "1882                 -0.10\n",
       "1883                 -0.18\n",
       "1884                 -0.27"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 1\n",
    "oceans = pd.read_json(data_folder_path / \"ocenas.json\", orient=\"columns\")\n",
    "oceans = oceans.drop(columns=\"description\")\n",
    "oceans = oceans.drop([\"title\", \"units\", \"base_period\", \"missing\"])\n",
    "oceans.index.name = \"year\"\n",
    "oceans = oceans.rename(columns={\"data\": \"temp_anomaly_celsius\"})\n",
    "oceans.index = pd.to_datetime(oceans.index).year\n",
    "oceans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189512</th>\n",
       "      <td>50.34</td>\n",
       "      <td>-1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189612</th>\n",
       "      <td>51.99</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189712</th>\n",
       "      <td>51.56</td>\n",
       "      <td>-0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189812</th>\n",
       "      <td>51.43</td>\n",
       "      <td>-0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189912</th>\n",
       "      <td>51.01</td>\n",
       "      <td>-1.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        value  anomaly\n",
       "189512  50.34    -1.68\n",
       "189612  51.99    -0.03\n",
       "189712  51.56    -0.46\n",
       "189812  51.43    -0.59\n",
       "189912  51.01    -1.01"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 2\n",
    "import json\n",
    "\n",
    "temp_file_path = data_folder_path / \"temp_temperatures.json\"\n",
    "\n",
    "with (data_folder_path / \"temperatures.json\").open(\"rt\") as f:\n",
    "    raw_data = json.load(f)\n",
    "\n",
    "raw_data = raw_data[\"data\"]\n",
    "\n",
    "with temp_file_path.open(\"wt\") as f:\n",
    "    json.dump(raw_data, f, indent=4)\n",
    "\n",
    "data = pd.read_json(temp_file_path, orient=\"index\")\n",
    "\n",
    "# remove the file\n",
    "temp_file_path.unlink()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nametype</th>\n",
       "      <th>recclass</th>\n",
       "      <th>mass</th>\n",
       "      <th>fall</th>\n",
       "      <th>year</th>\n",
       "      <th>reclat</th>\n",
       "      <th>reclong</th>\n",
       "      <th>coordinates_x</th>\n",
       "      <th>coordinates_y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aachen</th>\n",
       "      <td>Valid</td>\n",
       "      <td>L5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>50.77500</td>\n",
       "      <td>6.08333</td>\n",
       "      <td>6.08333</td>\n",
       "      <td>50.77500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aarhus</th>\n",
       "      <td>Valid</td>\n",
       "      <td>H6</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>56.18333</td>\n",
       "      <td>10.23333</td>\n",
       "      <td>10.23333</td>\n",
       "      <td>56.18333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Abee</th>\n",
       "      <td>Valid</td>\n",
       "      <td>EH4</td>\n",
       "      <td>107000.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1952.0</td>\n",
       "      <td>54.21667</td>\n",
       "      <td>-113.00000</td>\n",
       "      <td>-113.00000</td>\n",
       "      <td>54.21667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acapulco</th>\n",
       "      <td>Valid</td>\n",
       "      <td>Acapulcoite</td>\n",
       "      <td>1914.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>16.88333</td>\n",
       "      <td>-99.90000</td>\n",
       "      <td>-99.90000</td>\n",
       "      <td>16.88333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Achiras</th>\n",
       "      <td>Valid</td>\n",
       "      <td>L6</td>\n",
       "      <td>780.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>-33.16667</td>\n",
       "      <td>-64.95000</td>\n",
       "      <td>-64.95000</td>\n",
       "      <td>-33.16667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         nametype     recclass      mass  fall    year    reclat    reclong  \\\n",
       "name                                                                          \n",
       "Aachen      Valid           L5      21.0  Fell  1880.0  50.77500    6.08333   \n",
       "Aarhus      Valid           H6     720.0  Fell  1951.0  56.18333   10.23333   \n",
       "Abee        Valid          EH4  107000.0  Fell  1952.0  54.21667 -113.00000   \n",
       "Acapulco    Valid  Acapulcoite    1914.0  Fell  1976.0  16.88333  -99.90000   \n",
       "Achiras     Valid           L6     780.0  Fell  1902.0 -33.16667  -64.95000   \n",
       "\n",
       "          coordinates_x  coordinates_y  \n",
       "name                                    \n",
       "Aachen          6.08333       50.77500  \n",
       "Aarhus         10.23333       56.18333  \n",
       "Abee         -113.00000       54.21667  \n",
       "Acapulco      -99.90000       16.88333  \n",
       "Achiras       -64.95000      -33.16667  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 3\n",
    "cities = pd.read_json(data_folder_path / \"cities.json\", orient=\"records\", dtype={\"nametype\": \"category\", \"recclass\": \"category\", \"fall\": \"category\"})\n",
    "coordinates = pd.json_normalize(cities[\"geolocation\"].to_list())[\"coordinates\"]\n",
    "cities[\"coordinates_x\"] = coordinates.str[0]\n",
    "cities[\"coordinates_y\"] = coordinates.str[1]\n",
    "cities = cities.set_index(\"name\")\n",
    "cities = cities.drop(columns=[\"geolocation\", \":@computed_region_cbhk_fwbd\", \":@computed_region_nnqa_25f4\", \"id\"])\n",
    "cities[\"year\"] = cities[\"year\"].astype(str).str[:4].astype(float)\n",
    "cities.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, Aachen to Tomakovka\n",
      "Data columns (total 9 columns):\n",
      " #   Column         Non-Null Count  Dtype   \n",
      "---  ------         --------------  -----   \n",
      " 0   nametype       1000 non-null   category\n",
      " 1   recclass       1000 non-null   category\n",
      " 2   mass           972 non-null    float64 \n",
      " 3   fall           1000 non-null   category\n",
      " 4   year           999 non-null    float64 \n",
      " 5   reclat         988 non-null    float64 \n",
      " 6   reclong        988 non-null    float64 \n",
      " 7   coordinates_x  988 non-null    float64 \n",
      " 8   coordinates_y  988 non-null    float64 \n",
      "dtypes: category(3), float64(6)\n",
      "memory usage: 62.8+ KB\n"
     ]
    }
   ],
   "source": [
    "cities.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas.json_normalize](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html): Normalize semi-structured JSON data into a flat table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spent</th>\n",
       "      <th>tx_index</th>\n",
       "      <th>type</th>\n",
       "      <th>addr</th>\n",
       "      <th>value</th>\n",
       "      <th>n</th>\n",
       "      <th>script</th>\n",
       "      <th>time</th>\n",
       "      <th>relayed_by</th>\n",
       "      <th>vout_sz</th>\n",
       "      <th>hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1H7r57SXAwaKs3Tf5ugbkRNxwfh9YaxC5b</td>\n",
       "      <td>7541</td>\n",
       "      <td>0</td>\n",
       "      <td>76a914b0cd787a7a879ac0a5277b0013ec7b11c145055d...</td>\n",
       "      <td>1586376721</td>\n",
       "      <td>0.0.0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0f06714015f334626a168ee3e0aa5e0d3866a33dad504b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1BPULhbGfrojrknyD7aZYMtRVUu38Cn75j</td>\n",
       "      <td>1364400</td>\n",
       "      <td>1</td>\n",
       "      <td>76a91471f13b222426eb80b47d2413d21a8904ec1966b2...</td>\n",
       "      <td>1586376721</td>\n",
       "      <td>0.0.0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0f06714015f334626a168ee3e0aa5e0d3866a33dad504b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1LQ6YURobx4EGZRp8bdEDHup6T56o5NGKN</td>\n",
       "      <td>3127836</td>\n",
       "      <td>0</td>\n",
       "      <td>76a914d4c895721d3a8cd74bb3ccbb699a3dbe342c0807...</td>\n",
       "      <td>1586376722</td>\n",
       "      <td>0.0.0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3684072a50d7389933210d7adf4f98640d3d53c8cb245e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1HSLVVSSQmzaNG8sbakhFDrmpzUPZLnYCe</td>\n",
       "      <td>30036732</td>\n",
       "      <td>1</td>\n",
       "      <td>76a914b44cae99837337275d21d2c5c6ed6cddf7a7e9f7...</td>\n",
       "      <td>1586376722</td>\n",
       "      <td>0.0.0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3684072a50d7389933210d7adf4f98640d3d53c8cb245e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3Lb2MJWbBE88BUHf6tAw8ZzhkR6H2cYRhR</td>\n",
       "      <td>206183</td>\n",
       "      <td>0</td>\n",
       "      <td>a914cf48401e3cf81080352f281ea859ccabd51a821487</td>\n",
       "      <td>1586376721</td>\n",
       "      <td>0.0.0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3d3cc141654170060a7e298a9e5298557970e8cd0051ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spent  tx_index  type                                addr     value  n  \\\n",
       "0  False         0     0  1H7r57SXAwaKs3Tf5ugbkRNxwfh9YaxC5b      7541  0   \n",
       "1  False         0     0  1BPULhbGfrojrknyD7aZYMtRVUu38Cn75j   1364400  1   \n",
       "2  False         0     0  1LQ6YURobx4EGZRp8bdEDHup6T56o5NGKN   3127836  0   \n",
       "3  False         0     0  1HSLVVSSQmzaNG8sbakhFDrmpzUPZLnYCe  30036732  1   \n",
       "4  False         0     0  3Lb2MJWbBE88BUHf6tAw8ZzhkR6H2cYRhR    206183  0   \n",
       "\n",
       "                                              script        time relayed_by  \\\n",
       "0  76a914b0cd787a7a879ac0a5277b0013ec7b11c145055d...  1586376721    0.0.0.0   \n",
       "1  76a91471f13b222426eb80b47d2413d21a8904ec1966b2...  1586376721    0.0.0.0   \n",
       "2  76a914d4c895721d3a8cd74bb3ccbb699a3dbe342c0807...  1586376722    0.0.0.0   \n",
       "3  76a914b44cae99837337275d21d2c5c6ed6cddf7a7e9f7...  1586376722    0.0.0.0   \n",
       "4     a914cf48401e3cf81080352f281ea859ccabd51a821487  1586376721    0.0.0.0   \n",
       "\n",
       "  vout_sz                                               hash  \n",
       "0       2  0f06714015f334626a168ee3e0aa5e0d3866a33dad504b...  \n",
       "1       2  0f06714015f334626a168ee3e0aa5e0d3866a33dad504b...  \n",
       "2       2  3684072a50d7389933210d7adf4f98640d3d53c8cb245e...  \n",
       "3       2  3684072a50d7389933210d7adf4f98640d3d53c8cb245e...  \n",
       "4       3  3d3cc141654170060a7e298a9e5298557970e8cd0051ab...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example 4\n",
    "with (data_folder_path / \"transactions.json\").open(\"rt\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "trans = pd.json_normalize(data[\"txs\"], record_path=[\"out\"], meta=[\"time\", \"relayed_by\", \"vout_sz\", \"hash\"])\n",
    "trans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas provides the `read_excel()` function to read data from Excel files into DataFrames. This function supports various parameters to customize the reading process.\n",
    "\n",
    "To facilitate working with multiple sheets from the same file, the ExcelFile class can be used to wrap the file and can be passed into read_excel There will be a performance benefit for reading multiple sheets as the file is read into memory only once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sheet_names property will generate a list of the sheet names in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2002', '2004']\n"
     ]
    }
   ],
   "source": [
    "# Assign spreadsheet filename: file\n",
    "file = data_folder_path / \"battledeath.xlsx\"\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print xlssheet names\n",
    "print(xls.sheet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read an Excel file into a pandas DataFrame.\n",
    "\n",
    "Supports xls, xlsx, xlsm, xlsb, and odf file extensions read from a local filesystem or URL. Supports an option to read a single sheet or a list of sheets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>War, age-adjusted mortality due to</th>\n",
       "      <th>2002</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>36.083990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>0.128908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>18.314120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Andorra</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angola</td>\n",
       "      <td>18.964560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  War, age-adjusted mortality due to       2002\n",
       "0                        Afghanistan  36.083990\n",
       "1                            Albania   0.128908\n",
       "2                            Algeria  18.314120\n",
       "3                            Andorra   0.000000\n",
       "4                             Angola  18.964560"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2002 = pd.read_excel(xls, \"2002\")\n",
    "df_2002.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ExcelFile class can also be used as a context manager. The primary use-case for an ExcelFile is parsing multiple sheets with different parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelFile(file) as xls:\n",
    "    df_2002 = pd.read_excel(xls, \"2002\", names=[\"Country\", \"AAM due to War (2002)\"], index_col=\"Country\")\n",
    "    df_2004 = pd.read_excel(xls, \"2004\", names=[\"Country\", \"War(2004)\"], index_col=\"Country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAM due to War (2002)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>36.083990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>0.128908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             AAM due to War (2002)\n",
       "Country                           \n",
       "Afghanistan              36.083990\n",
       "Albania                   0.128908"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2002.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>War(2004)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Afghanistan</th>\n",
       "      <td>9.451028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albania</th>\n",
       "      <td>0.130354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             War(2004)\n",
       "Country               \n",
       "Afghanistan   9.451028\n",
       "Albania       0.130354"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2004.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XML files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML is a markup language designed to store and transport data, with a focus on simplicity and usability across different systems. pandas, a powerful data analysis library in Python, provides functions like `read_xml()` and `to_xml()` to read from and write to XML files, respectively. These functions enable data scientists and analysts to integrate XML data into their workflows seamlessly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an XML file named employees.xml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alice</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Engineering</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name   department\n",
       "id                    \n",
       "1   Alice           HR\n",
       "2     Bob  Engineering"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = data_folder_path / \"employees.xml\"\n",
    "data = pd.read_xml(path, xpath=\".//employee\")\n",
    "data = data.set_index(\"id\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading online data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online datasets are pre-collected data available in various formats and hosted on platforms dedicated to data sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have an URL with CSV data in raw format. You can read it directly into a pandas DataFrame using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Name</th>\n",
       "      <th>URL</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Awesome CSV Tools</td>\n",
       "      <td>NimbleText/Live</td>\n",
       "      <td>https://NimbleText.com/Live</td>\n",
       "      <td>Use patterns to manipulate CSV; the world's si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awesome CSV Tools</td>\n",
       "      <td>PapaParse</td>\n",
       "      <td>https://www.papaparse.com</td>\n",
       "      <td>A powerful in-browser CSV parser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Awesome CSV Tools</td>\n",
       "      <td>CSVKit</td>\n",
       "      <td>http://csvkit.readthedocs.org/en/0.7.3/</td>\n",
       "      <td>CSV utilities that includes csvsql / csvgrep /...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Awesome CSV Tools</td>\n",
       "      <td>XSV</td>\n",
       "      <td>https://github.com/BurntSushi/xsv</td>\n",
       "      <td>A fast CSV command-line toolkit written in Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Awesome CSV Tools</td>\n",
       "      <td>sed (gnu tool)</td>\n",
       "      <td>https://www.gnu.org/software/sed/manual/sed.html</td>\n",
       "      <td>Stream editor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Category             Name  \\\n",
       "0  Awesome CSV Tools  NimbleText/Live   \n",
       "1  Awesome CSV Tools        PapaParse   \n",
       "2  Awesome CSV Tools           CSVKit   \n",
       "3  Awesome CSV Tools              XSV   \n",
       "4  Awesome CSV Tools   sed (gnu tool)   \n",
       "\n",
       "                                                URL  \\\n",
       "0                       https://NimbleText.com/Live   \n",
       "1                         https://www.papaparse.com   \n",
       "2           http://csvkit.readthedocs.org/en/0.7.3/   \n",
       "3                 https://github.com/BurntSushi/xsv   \n",
       "4  https://www.gnu.org/software/sed/manual/sed.html   \n",
       "\n",
       "                                         Description  \n",
       "0  Use patterns to manipulate CSV; the world's si...  \n",
       "1                   A powerful in-browser CSV parser  \n",
       "2  CSV utilities that includes csvsql / csvgrep /...  \n",
       "3    A fast CSV command-line toolkit written in Rust  \n",
       "4                                      Stream editor  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/secretGeek/AwesomeCSV/refs/heads/master/awesomecsv.csv\"\n",
    "data = pd.read_csv(url)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting and writing data to files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is not only excellent for data manipulation and analysis but also for exporting data to various file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>revenues</th>\n",
       "      <th>revenue_change</th>\n",
       "      <th>profits</th>\n",
       "      <th>assets</th>\n",
       "      <th>profit_change</th>\n",
       "      <th>ceo</th>\n",
       "      <th>industry</th>\n",
       "      <th>sector</th>\n",
       "      <th>previous_rank</th>\n",
       "      <th>country</th>\n",
       "      <th>hq_location</th>\n",
       "      <th>website</th>\n",
       "      <th>years_on_global_500_list</th>\n",
       "      <th>employees</th>\n",
       "      <th>total_stockholder_equity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>company</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Walmart</th>\n",
       "      <td>1</td>\n",
       "      <td>485873</td>\n",
       "      <td>0.8</td>\n",
       "      <td>13643.0</td>\n",
       "      <td>198825</td>\n",
       "      <td>-7.2</td>\n",
       "      <td>C. Douglas McMillon</td>\n",
       "      <td>General Merchandisers</td>\n",
       "      <td>Retailing</td>\n",
       "      <td>1</td>\n",
       "      <td>USA</td>\n",
       "      <td>Bentonville, AR</td>\n",
       "      <td>http://www.walmart.com</td>\n",
       "      <td>23</td>\n",
       "      <td>2300000</td>\n",
       "      <td>77798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State Grid</th>\n",
       "      <td>2</td>\n",
       "      <td>315199</td>\n",
       "      <td>-4.4</td>\n",
       "      <td>9571.3</td>\n",
       "      <td>489838</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>Kou Wei</td>\n",
       "      <td>Utilities</td>\n",
       "      <td>Energy</td>\n",
       "      <td>2</td>\n",
       "      <td>China</td>\n",
       "      <td>Beijing, China</td>\n",
       "      <td>http://www.sgcc.com.cn</td>\n",
       "      <td>17</td>\n",
       "      <td>926067</td>\n",
       "      <td>209456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sinopec Group</th>\n",
       "      <td>3</td>\n",
       "      <td>267518</td>\n",
       "      <td>-9.1</td>\n",
       "      <td>1257.9</td>\n",
       "      <td>310726</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>Wang Yupu</td>\n",
       "      <td>Petroleum Refining</td>\n",
       "      <td>Energy</td>\n",
       "      <td>4</td>\n",
       "      <td>China</td>\n",
       "      <td>Beijing, China</td>\n",
       "      <td>http://www.sinopec.com</td>\n",
       "      <td>19</td>\n",
       "      <td>713288</td>\n",
       "      <td>106523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>China National Petroleum</th>\n",
       "      <td>4</td>\n",
       "      <td>262573</td>\n",
       "      <td>-12.3</td>\n",
       "      <td>1867.5</td>\n",
       "      <td>585619</td>\n",
       "      <td>-73.7</td>\n",
       "      <td>Zhang Jianhua</td>\n",
       "      <td>Petroleum Refining</td>\n",
       "      <td>Energy</td>\n",
       "      <td>3</td>\n",
       "      <td>China</td>\n",
       "      <td>Beijing, China</td>\n",
       "      <td>http://www.cnpc.com.cn</td>\n",
       "      <td>17</td>\n",
       "      <td>1512048</td>\n",
       "      <td>301893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toyota Motor</th>\n",
       "      <td>5</td>\n",
       "      <td>254694</td>\n",
       "      <td>7.7</td>\n",
       "      <td>16899.3</td>\n",
       "      <td>437575</td>\n",
       "      <td>-12.3</td>\n",
       "      <td>Akio Toyoda</td>\n",
       "      <td>Motor Vehicles and Parts</td>\n",
       "      <td>Motor Vehicles &amp; Parts</td>\n",
       "      <td>8</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Toyota, Japan</td>\n",
       "      <td>http://www.toyota-global.com</td>\n",
       "      <td>23</td>\n",
       "      <td>364445</td>\n",
       "      <td>157210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          rank  revenues  revenue_change  profits  assets  \\\n",
       "company                                                                     \n",
       "Walmart                      1    485873             0.8  13643.0  198825   \n",
       "State Grid                   2    315199            -4.4   9571.3  489838   \n",
       "Sinopec Group                3    267518            -9.1   1257.9  310726   \n",
       "China National Petroleum     4    262573           -12.3   1867.5  585619   \n",
       "Toyota Motor                 5    254694             7.7  16899.3  437575   \n",
       "\n",
       "                          profit_change                  ceo  \\\n",
       "company                                                        \n",
       "Walmart                            -7.2  C. Douglas McMillon   \n",
       "State Grid                         -6.2              Kou Wei   \n",
       "Sinopec Group                     -65.0            Wang Yupu   \n",
       "China National Petroleum          -73.7        Zhang Jianhua   \n",
       "Toyota Motor                      -12.3          Akio Toyoda   \n",
       "\n",
       "                                          industry                  sector  \\\n",
       "company                                                                      \n",
       "Walmart                      General Merchandisers               Retailing   \n",
       "State Grid                               Utilities                  Energy   \n",
       "Sinopec Group                   Petroleum Refining                  Energy   \n",
       "China National Petroleum        Petroleum Refining                  Energy   \n",
       "Toyota Motor              Motor Vehicles and Parts  Motor Vehicles & Parts   \n",
       "\n",
       "                          previous_rank country      hq_location  \\\n",
       "company                                                            \n",
       "Walmart                               1     USA  Bentonville, AR   \n",
       "State Grid                            2   China   Beijing, China   \n",
       "Sinopec Group                         4   China   Beijing, China   \n",
       "China National Petroleum              3   China   Beijing, China   \n",
       "Toyota Motor                          8   Japan    Toyota, Japan   \n",
       "\n",
       "                                               website  \\\n",
       "company                                                  \n",
       "Walmart                         http://www.walmart.com   \n",
       "State Grid                      http://www.sgcc.com.cn   \n",
       "Sinopec Group                   http://www.sinopec.com   \n",
       "China National Petroleum        http://www.cnpc.com.cn   \n",
       "Toyota Motor              http://www.toyota-global.com   \n",
       "\n",
       "                          years_on_global_500_list  employees  \\\n",
       "company                                                         \n",
       "Walmart                                         23    2300000   \n",
       "State Grid                                      17     926067   \n",
       "Sinopec Group                                   19     713288   \n",
       "China National Petroleum                        17    1512048   \n",
       "Toyota Motor                                    23     364445   \n",
       "\n",
       "                          total_stockholder_equity  \n",
       "company                                             \n",
       "Walmart                                      77798  \n",
       "State Grid                                  209456  \n",
       "Sinopec Group                               106523  \n",
       "China National Petroleum                    301893  \n",
       "Toyota Motor                                157210  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f500 = pd.read_csv(data_folder_path / \"f500.csv\", index_col=0)\n",
    "f500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV but with a different separator (;)\n",
    "f500.to_csv(data_folder_path / \"f500_semicolon.csv\", sep=\";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to JSON\n",
    "f500.to_json(data_folder_path / \"f500.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to Excel\n",
    "excel = pd.ExcelWriter(data_folder_path / \"f500.xlsx\")\n",
    "f500.to_excel(excel, sheet_name=\"f500\")\n",
    "excel.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to HTML\n",
    "f500.to_html(data_folder_path / \"f500.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to XML\n",
    "f500.to_xml(data_folder_path / \"f500.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all created files\n",
    "(data_folder_path / \"f500_semicolon.csv\").unlink(missing_ok=True)\n",
    "(data_folder_path / \"f500.json\").unlink(missing_ok=True)\n",
    "(data_folder_path / \"f500.xlsx\").unlink(missing_ok=True)\n",
    "(data_folder_path / \"f500.html\").unlink(missing_ok=True)\n",
    "(data_folder_path / \"f500.xml\").unlink(missing_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Pickle Format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pickle in Python: Object Serialization](https://www.datacamp.com/community/tutorials/pickle-python-tutorial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle is used for serializing and de-serializing Python object structures, also called marshalling or flattening. Serialization refers to the process of converting an object in memory to a byte stream that can be stored on disk or sent over a network. Later on, this character stream can then be retrieved and de-serialized back to a Python object. Pickling is not to be confused with compression! The former is the conversion of an object from one representation (data in Random Access Memory (RAM)) to another (text on disk), while the latter is the process of encoding data with fewer bits, in order to save disk space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling is useful for applications where you need some degree of persistency in your data. Your program's state data can be saved to disk, so you can continue working on it later on. It can also be used to send data over a Transmission Control Protocol (TCP) or socket connection, or to store python objects in a database. Pickle is very useful for when you're working with machine learning algorithms, where you want to save them to be able to make new predictions at a later time, without having to rewrite everything or train the model all over again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use data across different programming languages, pickle is not recommended. Its protocol is specific to Python, thus, cross-language compatibility is not guaranteed. The same holds for different versions of Python itself. Unpickling a file that was pickled in a different version of Python may not always work properly, so you have to make sure that you're using the same version and perform an update if necessary. You should also try not to unpickle data from an untrusted source. Malicious code inside the file might be executed upon unpickling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- File type native to Python\n",
    "- Motivation: many datatypes for which it isn’t obvious how to store them\n",
    "- Pickled files are serialized\n",
    "- Serialize = convert object to bytestream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of datatypes that cannot be saved easily to flat files, such as lists and dictionaries. If you want your files to be human readable, you may want to save them as text files in a clever manner. JSONs, which you will see in a later chapter, are appropriate for Python dictionaries.\n",
    "\n",
    "However, if you merely want to be able to import them into Python, you can serialize them. All this means is converting the object into a sequence of bytes, or a bytestream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pripravimo datoteko za pisnaje v pickle format\n",
    "titanic = pd.read_csv(\n",
    "    \"data/titanic_sub.csv\",\n",
    "    index_col=\"PassengerId\",\n",
    "    usecols=[\"PassengerId\", \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Cabin\", \"Embarked\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass     Sex   Age     Fare Cabin Embarked\n",
       "PassengerId                                                        \n",
       "1                   0       3    male  22.0   7.2500   NaN        S\n",
       "2                   1       1  female  38.0  71.2833   C85        C\n",
       "3                   1       3  female  26.0   7.9250   NaN        S"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas.DataFrame.to_pickle](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_pickle.html#pandas.DataFrame.to_pickle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.to_pickle(\"data/titanic_sub.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pickled pandas object (or any object) from file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pandas.read_pickle](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_pickle.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_read = pd.read_pickle(\"data/titanic_sub.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass     Sex   Age     Fare Cabin Embarked\n",
       "PassengerId                                                        \n",
       "1                   0       3    male  22.0   7.2500   NaN        S\n",
       "2                   1       1  female  38.0  71.2833   C85        C\n",
       "3                   1       3  female  26.0   7.9250   NaN        S\n",
       "4                   1       1  female  35.0  53.1000  C123        S\n",
       "5                   0       3    male  35.0   8.0500   NaN        S"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_read.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data from APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Fetch data from REST APIs and parse results into pandas.\n",
    "Topics:\n",
    "\n",
    "REST API fundamentals (endpoints, methods, status codes)\n",
    "\n",
    "Authentication: API keys, OAuth 2.0\n",
    "\n",
    "Using requests library: GET/POST, headers, pagination\n",
    "\n",
    "Handling rate limits and error codes\n",
    "Tutorial:\n",
    "\n",
    "Fetch weather data from OpenWeatherMap API.\n",
    "\n",
    "Paginate through GitHub Issues API and combine results.\n",
    "Exercise: Build a DataFrame of trending YouTube videos using YouTube Data API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Extract structured data from websites.\n",
    "Topics:\n",
    "\n",
    "HTML/CSS basics: tags, classes, IDs\n",
    "\n",
    "Ethical scraping: robots.txt, rate limiting\n",
    "\n",
    "Tools: BeautifulSoup, requests, pandas.read_html()\n",
    "Tutorial:\n",
    "\n",
    "Scrape Wikipedia tables into DataFrames with pd.read_html().\n",
    "\n",
    "Use BeautifulSoup to extract product prices from an e-commerce site.\n",
    "Exercise: Scrape real estate listings and calculate average prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many organizations store data in databases, which can be queried to extract exactly the information needed.\n",
    "\n",
    "Types of Databases\n",
    "- Relational Databases: Use SQL for querying (e.g., MySQL, PostgreSQL, SQLite).\n",
    "- NoSQL Databases: Designed for unstructured data (e.g., MongoDB, Cassandra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data & Cloud Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Access large datasets from cloud storage.\n",
    "Topics:\n",
    "\n",
    "Cloud storage: AWS S3, Google Cloud Storage\n",
    "\n",
    "Parquet/Feather formats for efficient I/O\n",
    "\n",
    "Using boto3 to read from S3\n",
    "Tutorial:\n",
    "\n",
    "Load a 1GB Parquet file from S3 using s3fs and pd.read_parquet().\n",
    "\n",
    "Compare read speeds for CSV vs. Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Data Acquisition Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Combine multiple data sources into a unified dataset.\n",
    "Requirements:\n",
    "\n",
    "Scrape product data from a website.\n",
    "\n",
    "Fetch complementary pricing data via API.\n",
    "\n",
    "Merge with historical sales data from a SQL database.\n",
    "\n",
    "Validate, clean, and export to Parquet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
